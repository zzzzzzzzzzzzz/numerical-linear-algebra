{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy import linalg\n",
    "from scipy import sparse as sp\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Sample graph of friends in social network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('network.npy').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<128x128 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2048 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = sp.csr_matrix(data)\n",
    "adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing spectral clustering (to predict links in graph)\n",
    "\n",
    "**Based on https://link.springer.com/content/pdf/10.1007%2Fs10618-010-0210-x.pdf**\n",
    "\n",
    "## 1. Construct the graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to construct the Laplacian for the given graph. \n",
    "\n",
    "Given the **adjacency matrix** $A \\in \\mathbb{R}^{N \\times N},$ we define the **degree matrix** $D \\in \\mathbb{R}^{N \\times N}$ of an undirected graph as\n",
    "$$D_{ij} =  \\begin{cases}\\sum_{k=1}^N A_{ik} & if \\;\\; i = j\\\\ 0 & if \\;\\; i \\ne j\\end{cases}$$\n",
    "\n",
    "Our goal is to minimze the **normalized cut**, we will need to use the **normalized Laplacian** (a.k.a. symmetrized Laplacian), defined as\n",
    "$$L_{sym} = I - D^{-1/2}AD^{-1/2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import laplacian\n",
    "import scipy\n",
    "\n",
    "def construct_laplacian(A, norm_laplacian):\n",
    "    \"\"\"Construct Laplacian of a graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    norm_laplacian : bool\n",
    "        Whether to construct the normalized graph Laplacian or not.\n",
    "        If True, construct the normalized (symmetrized) Laplacian, L = I - D^{-1/2} A D^{-1/2}.\n",
    "        If False, construct the unnormalized Laplacian, L = D - A.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    L : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Laplacian of the graph.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if norm_laplacian:\n",
    "        D_diag = sp.csr_matrix(sp.diags([np.sqrt(1/np.sum(row)) for row in A]))\n",
    "        \n",
    "        return scipy.sparse.csr_matrix(sp.eye(*A.shape) -  D_diag.dot(A.dot(D_diag)))\n",
    "    else:\n",
    "        return sp.diags([np.sum(row) for row in A]) - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# get_L = partial(construct_laplacian, norm_laplacian=False)\n",
    "get_L_sym = partial(construct_laplacian, norm_laplacian=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectral embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to compute the spectral embedding for the given graph.\n",
    "\n",
    "In order to partition the graph into $k$ clusters, such that the desired cut (ratio or normalized) is minimized, we need to consider the $k$ eigenvectors corresponding to the $k$ smallest eigenvalues of the graph Laplacian.\n",
    "\n",
    "Since the Laplacian matrix is sparse and symmetric, we can use the function `eigsh` from the `scipy.sparse.linalg` package in order to find eigendecomposition of $L$ (`eig` - eigendecomposition, `s` - sparse, `h`- Hermitian).\n",
    "The function `eigsh` directly allows you to find the smallest / largest eigenvalues by specifying the `k` and `which` parameters. \n",
    "\n",
    "Keep in mind that the Laplacian matrix is always positive semi-definite when picking the appropriate value for the `which` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_embedding(A, num_clusters, get_L_func=get_L_sym):\n",
    "    \"\"\"Compute spectral embedding of nodes in the given graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    num_clusters : int\n",
    "        Number of clusters to detect in the data.\n",
    "    get_L_func : callable, default get_L\n",
    "        Function to calcualte Lapassian for A matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    embedding : np.array, shape [N, num_clusters]\n",
    "        Spectral embedding for the given graph.\n",
    "        Each row represents the spectral embedding of a given node.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    L_ = get_L_func(A)\n",
    "    eigvals, eigenvecs = eigsh(L_, k=num_clusters+1, which='SA')\n",
    "    \n",
    "    return eigenvecs[...,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_embedding(adj, 10, get_L_sym).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate positive and negative samples for link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "adj_dense = adj.todense()\n",
    "for i in range(adj_dense.shape[0]):\n",
    "    for j in range(i, adj_dense.shape[1]):\n",
    "        if adj_dense[i,j] == 1:\n",
    "            positive.append((i,j))\n",
    "        else:\n",
    "            negative.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = np.array(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = np.array(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = negative[random.sample(range(0, len(negative)), len(positive))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate features for edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this paper https://arxiv.org/pdf/1607.00653.pdf it's better to choose L1 norm (hence tests on more complicated datasets showed that for spectral clustering L1 is the best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "embed = spectral_embedding(adj, k, get_L_sym)\n",
    "\n",
    "def wL1(a,b):\n",
    "    return np.abs(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for elem in positive:\n",
    "    features.append(wL1(embed[elem[0]], embed[elem[1]]))\n",
    "\n",
    "pos_features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for elem in negative:\n",
    "    features.append(wL1(embed[elem[0]], embed[elem[1]]))\n",
    "\n",
    "neg_features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.concatenate((pos_features, neg_features), axis=0)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((np.ones(pos_features.shape[0]), np.zeros(neg_features.shape[0])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 8.47 seconds for 12 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.887 (std: 0.023)\n",
      "Parameters: {'C': 1, 'kernel': 'poly'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.887 (std: 0.023)\n",
      "Parameters: {'C': 2, 'kernel': 'poly'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.887 (std: 0.023)\n",
      "Parameters: {'C': 3, 'kernel': 'poly'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = features\n",
    "y = labels\n",
    "# build a classifier\n",
    "clf = svm.SVC(gamma='auto')\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"C\": [1, 2, 3],\n",
    "              \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "             }\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=skf.split(X,y), scoring='roc_auc')\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
